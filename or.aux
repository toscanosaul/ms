\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{robbins}
\citation{sgd:Nemirovski}
\citation{adam}
\citation{cnn_cifar}
\citation{murphy2012machine}
\citation{fu2015}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{schoen:1991}
\citation{marti:2016}
\citation{kushner}
\citation{MLS}
\@writefile{toc}{\contentsline {section}{\numberline {1}INTRODUCTION}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{INTRODUCTION}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]1}{2}}
\citation{Swersky:2014}
\citation{Hutter:2015}
\citation{kim:20062}
\citation{Swersky:2014}
\citation{Hutter:2015}
\citation{yoshua94}
\citation{multi_start}
\citation{boender:1987}
\citation{frazier2009calibration}
\citation{chen2016}
\citation{Swersky:2014}
\citation{Hutter:2015}
\citation{klein2016fast}
\citation{boca2017}
\citation{li2016}
\citation{Swersky:2014}
\citation{Hutter:2015}
\citation{klein2016fast}
\citation{Hutter:2015}
\citation{Swersky:2014}
\citation{klein2016fast}
\citation{Swersky:2014}
\citation{Swersky:2014}
\citation{Hutter:2015}
\citation{Hutter:2015}
\@writefile{toc}{\contentsline {paragraph}{Related Work:}{4}{section*.1}}
\citation{Hutter:2015}
\citation{klein2016fast}
\citation{li2016}
\citation{Swersky:2014}
\citation{Swersky:2014}
\citation{entropySearch}
\citation{jones1998efficient}
\citation{forrester2008engineering}
\citation{brochu2010tutorial}
\citation{frazier2009knowledge}
\citation{brochu2010tutorial}
\citation{Bechhofer:1995}
\citation{kim:2007}
\citation{kim:20062}
\citation{frazier2009knowledge}
\citation{powell2007approximate}
\@writefile{toc}{\contentsline {section}{\numberline {2}THE SGD-GP STATISTICAL MODEL ON THE PATH OF SGD}{6}{section.2}}
\newlabel{statistical_model}{{2}{6}{THE SGD-GP STATISTICAL MODEL ON THE PATH OF SGD}{section.2}{}}
\newlabel{statistical_model@cref}{{[section][2][]2}{6}}
\newlabel{eq:SGD}{{1}{6}{THE SGD-GP STATISTICAL MODEL ON THE PATH OF SGD}{equation.2.1}{}}
\citation{kushner}
\citation{Neal:GPBayesian}
\citation{glasserman:mc}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Inference Over $M(n)$ in One-Dimension Given Hyperparameter $\theta $}{7}{subsection.2.1}}
\newlabel{sec:SGD-GP-1}{{2.1}{7}{Inference Over $M(n)$ in One-Dimension Given Hyperparameter $\theta $}{subsection.2.1}{}}
\newlabel{sec:SGD-GP-1@cref}{{[subsection][1][2]2.1}{7}}
\citation{radford2003Slice}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Inference Over $M(n)$ in Multiple Dimensions, Marginalizing over Hyperparameter $\theta $}{8}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Inference Over $f(X_\infty )$}{9}{subsection.2.3}}
\newlabel{sec:SGD-GP-2}{{2.3}{9}{Inference Over $f(X_\infty )$}{subsection.2.3}{}}
\newlabel{sec:SGD-GP-2@cref}{{[subsection][3][2]2.3}{9}}
\newlabel{eq:a}{{2}{9}{Inference Over $f(X_\infty )$}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}THE MOST LIKELY TO SUCCEED ALLOCATION RULE}{10}{section.3}}
\newlabel{MLS}{{3}{10}{THE MOST LIKELY TO SUCCEED ALLOCATION RULE}{section.3}{}}
\newlabel{MLS@cref}{{[section][3][]3}{10}}
\citation{Swersky:2014}
\citation{Hutter:2015}
\@writefile{toc}{\contentsline {section}{\numberline {4}NUMERICAL EXPERIMENTS}{12}{section.4}}
\newlabel{experiments}{{4}{12}{NUMERICAL EXPERIMENTS}{section.4}{}}
\newlabel{experiments@cref}{{[section][4][]4}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A Concave Objective Function}{12}{subsection.4.1}}
\newlabel{concave}{{4.1}{12}{A Concave Objective Function}{subsection.4.1}{}}
\newlabel{concave@cref}{{[subsection][1][4]4.1}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  The SGD-GP statistical model (b) and MLS\ allocation rule (c) on the problem (a) from $\mathsection $\ref  {concave}. (b) shows that SGD-GP can predict the start's limiting objective value with high precision after 10 iterations. (c) shows that our policy finds the optimal solution faster than the equal allocation policy. \relax }}{13}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:quadratic}{{1}{13}{The SGD-GP statistical model (b) and \abbrv \ allocation rule (c) on the problem (a) from $\mathsection $\ref {concave}. (b) shows that SGD-GP can predict the start's limiting objective value with high precision after 10 iterations. (c) shows that our policy finds the optimal solution faster than the equal allocation policy. \relax }{figure.caption.2}{}}
\newlabel{fig:quadratic@cref}{{[figure][1][]1}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}An Objective Function with Many Local Maxima}{13}{subsection.4.2}}
\newlabel{local_maximum}{{4.2}{13}{An Objective Function with Many Local Maxima}{subsection.4.2}{}}
\newlabel{local_maximum@cref}{{[subsection][2][4]4.2}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The SGD-GP statistical model (b) and MLS\ allocation rule (c) on the problem (a) from $\mathsection $\ref  {local_maximum}. (b) shows that SGD-GP can predict the start's limiting objective value with high precision after 20 iterations. (c) shows that our policy does better than the other policies after only 8 iterations. \relax }}{14}{figure.caption.3}}
\newlabel{fig:multiple_maxima}{{2}{14}{The SGD-GP statistical model (b) and \abbrv \ allocation rule (c) on the problem (a) from $\mathsection $\ref {local_maximum}. (b) shows that SGD-GP can predict the start's limiting objective value with high precision after 20 iterations. (c) shows that our policy does better than the other policies after only 8 iterations. \relax }{figure.caption.3}{}}
\newlabel{fig:multiple_maxima@cref}{{[figure][2][]2}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Objective Function with a Vanishing Gradient}{14}{subsection.4.3}}
\newlabel{gradient_vanishing}{{4.3}{14}{Objective Function with a Vanishing Gradient}{subsection.4.3}{}}
\newlabel{gradient_vanishing@cref}{{[subsection][3][4]4.3}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  The SGD-GP statistical model (b) and MLS\ allocation rule (c) on the problem (a) from $\mathsection $\ref  {gradient_vanishing}. (b) shows that SGD-GP can predict a start's limiting objective value with high precision after 15 iterations. (c) shows that our policy allocates resources to promising starters since the beginning, while the other policies waste resources moving not promising starters.\relax }}{15}{figure.caption.4}}
\newlabel{fig:gradient_vanishing}{{3}{15}{The SGD-GP statistical model (b) and \abbrv \ allocation rule (c) on the problem (a) from $\mathsection $\ref {gradient_vanishing}. (b) shows that SGD-GP can predict a start's limiting objective value with high precision after 15 iterations. (c) shows that our policy allocates resources to promising starters since the beginning, while the other policies waste resources moving not promising starters.\relax }{figure.caption.4}{}}
\newlabel{fig:gradient_vanishing@cref}{{[figure][3][]3}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}The 20-Dimensional Rosenbrock Function}{16}{subsection.4.4}}
\newlabel{rosenbrock}{{4.4}{16}{The 20-Dimensional Rosenbrock Function}{subsection.4.4}{}}
\newlabel{rosenbrock@cref}{{[subsection][4][4]4.4}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Performance comparison between MLS\tmspace  +\thinmuskip {.1667em} equal allocation, and random allocation using $30$ starting points. Our policy does much better than the other policies after only 45 iterations. In fact, after only 100 iterations, our policy finds the global optimum, while the other policies are very far away from a good solution. This problem shows that our policy works extremely well on high-dimensional problems where the objective function is noisy. \relax }}{16}{figure.caption.5}}
\newlabel{fig:rosenbrock}{{4}{16}{Performance comparison between \abbrv \, equal allocation, and random allocation using $30$ starting points. Our policy does much better than the other policies after only 45 iterations. In fact, after only 100 iterations, our policy finds the global optimum, while the other policies are very far away from a good solution. This problem shows that our policy works extremely well on high-dimensional problems where the objective function is noisy. \relax }{figure.caption.5}{}}
\newlabel{fig:rosenbrock@cref}{{[figure][4][]4}{16}}
\citation{kushner}
\citation{kushner}
\citation{kushner}
\@writefile{toc}{\contentsline {section}{\numberline {5}CONCLUSION}{17}{section.5}}
\newlabel{conclusion}{{5}{17}{CONCLUSION}{section.5}{}}
\newlabel{conclusion@cref}{{[section][5][]5}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {A}APPENDICES}{17}{appendix.A}}
\newlabel{app:THEOREMS}{{A}{17}{APPENDICES}{appendix.A}{}}
\newlabel{app:THEOREMS@cref}{{[appendix][1][2147483647]A}{17}}
\citation{kushner}
\bibstyle{wsc}
\@input{bu.aux}
\bibdata{or}
